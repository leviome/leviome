# 为什么神经网络要有激活函数

激活函数是神经元不可缺少的部分，常见的激活函数有relu,tanh,sigmoid等等

想起刚学习deep learning的时候并没有特别在意这个激活函数，反正它就是神经元三要素之一就对了。

今天忽然被一位AI爱好者小姐姐问到这个问题，忽然发现自己一直没有想过这个问题。于是打算从数学理论的角度，来回答这个问题。

---


<div align="center">

![image](https://raw.githubusercontent.com/leviome/markdown-images/master/images/p1.png)
</div>
对于一个神经元而言，足以用通用公式

```
y = f(wx+b)，w,x为多维向量  
```

来表示它的数学原理，这里的f()就是激活函数。OK，设想去除f()的话，就变成了
```math
y = wx+b
```
是一种纯线性关系，也就是直男属性。
直男作为当今社会众矢之的，也是有原因的：
当神经网络层数叠加时，上一层的输出y变成了下一层的输入x，比如
```math
y1 = w1*x1+b1,
```
```math
y2 = w2*x2+b2 (x2 = y1),
```
则

```math
y2 = w2*(w1*x1+b1) + b2 = w1*w2*x1 + w2*b1+b2 = w'*x1 + b'
```
此时，y2也是x1的线性函数，那么神经网络层数增加，在这个时候是没有意义的。也就是说，再多层的神经网络在此时也只是相当于单层感知机。

神经网络的数学作用就是==拟合实际函数==

神经网络通过大量数据训练，通过(x,y)的灌输找到一个运算法则y=f(x)，也就是拟合出一个函数曲线与f(x)十分相似甚至一模一样的函数出来。于是，神经网络的好坏我们可以等价看成拟合曲线能力的强弱。
没有激活函数的单层感知机就是一个苍白的线性函数，它的拟合能力又如何呢？
y=wx+b，无论w和b如何变换，y都跟x成强线性相关。于是，得出结论：
没有激活函数的神经网络，相当于没有激活函数的单层感知机，这种钢铁直男产物只能拟合狭义线性曲线。。比如随便来一个分段函数他都拟合不了。
生活中几乎都是非线性函数，所以没有激活函数，根本没法玩儿。
激活函数对神经网络最基本的用处就是，提供nonlinearity，使得神经网络具有拟合非线性函数的能力。





